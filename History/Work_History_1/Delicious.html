<hr>

<h3 class="company first-color">Delicious</h3>

<table cellspacing="0">


   <tr>
		<td class="second-color"> Dates: </td>


<td> <i> Jun 2006 - Jun 2008 </i> </td>

	</tr>
 



   <tr>
		<td class="second-color"> Role: </td>

   	<td> <i> Developer </i> </td>
   </tr>



   <tr>
		<td class="second-color"> Technologies: </td>	
		<td> <i> apache, mysql </i> </td>
	</tr>



   <tr>
		<td class="second-color">Programming Languages: </td>

		<td> <i> c++, erlang, perl </i></td>
	</tr>
 


   <tr>
		<td class="second-color">DSLs: </td>

		<td> <i> make, sql </i></td>
	</tr>


</table>

<h4 class="project-item third-color">
	<i>Objectives</i>
</h4>

<div class="project-item-detail">
	<p>Provide a simple way to save URLS, associate them with tags.  Support publishing url sets via RSS, and social discovery.</p>
</div><h4 class="project-item third-color">
	<i>Responsibilites</i>
</h4>

<div class="project-item-detail">
	<p>I was responsible for guaranteeing reliable migration of the legacy data to the new data architecture.  I implemented the sql query generator for the new federated system.  I also implemented the migration system to support bi-directional migration between the original infrastructure and the new federated database layer.</p>
</div><h4 class="project-item third-color">
	<i>Key Challenges</i>
</h4>

<div class="project-item-detail">
	<p>Provide a logic to map incoming URL query strings to federation aware SQL queries.  Provide a robust mechanism for migrating the original data into the new federated system.  This solution needed to support data integrity testing, tear-down and rebuild of the new data sub-system and support launch-time scenarios.</p>
</div><h4 class="project-item third-color">
	<i>Technical Details</i>
</h4>

<div class="project-item-detail">
	<p><b>Data Migration</b></p>

<p>The database organization of the acquired company had all the warts and faults of a fast developing product.  In addition to inconsistent schema changes, there were inconsistencies in data formatting which were managed by a special layer of data munging code.  Additionally, different features on the site depended on different master/slave configurations.  The original system used over 30 database instances.  Our target was 6 clusters of master/slave triples requiring 18 databases.</p>

<p>
<img src="Delicious/Technical_Details.jpg" alt="" >
</p>

<p>
<img src="Delicious/Technical_Details1.jpg" alt="" >
</p>

	<ul><li>	Transform source data schema into target schema</li>
	<li>	Leverage existing data-scrubbing code - as implemented - to maintain data integrity.  </li></ul>
	<blockquote>⁃	This was achieved by running the Perl scrubbing code within Erlang processes as the data was read from the source infrastructure.</blockquote>
	<ul><li>	Dynamically scale source read / target write rates to ensure that processing hosts didn’t run out of memory due to queue backlogs.</li></ul>
	<blockquote>⁃	The Erlang VM on the processing hosts was instrumented to monitor machine memory and use an exponential back-off on the data readers to accommodate memory surges.</blockquote>
	<ul><li>	Provide robust - queryable log to track failed writes into the new installation</li></ul>
	<blockquote>⁃	Data processing logs were written to Mnesia which made it easy to aggregate processing results from multiple replication hosts and simple to query</blockquote>
	<ul><li>	Allow easy rebuilding of gold-masters as the federated database system was hardened.  We ran around 30 full migrations during testing and development.  A full migration could be achieved within ~24 hours.</li></ul>
	<blockquote>⁃	The fault tolerance capabilities of Erlang were critical, write operations were scoped to individual Erlang processes which localized failure and retry at the most granular level</blockquote>
	<ul><li>	Support a live replication from the original topology to the new system</li></ul>
	<blockquote>⁃	At launch the original database configurations handled writes and reads were served from the new infrastructure.  The migration logic maintained live parity between the two systems until stake holders were comfortable cutting writes over to the new system.</blockquote>
	<ul><li>	Provide a reverse replicator to write data from the new data sub-system to the original databases.</li></ul>
	<blockquote>⁃	This was a fail-safe.  Allow the team to cut back to the old system after writes had started flowing to the new databases.</blockquote>

<p><b>Query Generator</p>
</b><p>This code needed to map incoming URL query strings into the underlying - federation aware - SQL.  In the simple case a user was accessing their own data and this was a single query mapped to a single cluster.  Accessing tagged data from a user’s social network would require multiple concurrent queries.  There are a few more scenarios which I can no longer recall.  The query generator logic was executed in the Apache Servers providing the Federation Layer.</p>

</div><h4 class="project-item third-color">
	<i>Techniques used</i>
</h4>

<div class="project-item-detail">
	<p>There was existing legacy perl code which managed the various ‘special cases’ in the original data.  This code was running in production and needed to be incorporated into the migration logic.  At the same time this needed to be runnable in a parallel concurrent environment.  Erlang was used for the concurrency layer.  The production interface provided a log table of all transactions executed in production.  This ‘queue’ was consumed by migrator nodes.  The log tracked CREATE, UPDATE, DELETE operations for user histories.  A migrator node would consume the command history for a given user and replay the command sequence against the new federation layer.</p>
<p>There were multiple migrator nodes and each was assigned the set of users that mapped to a particular federation cluster.  These nodes needed to monitor host memory to slow down reads from the queue based on the performance characteristics of the data scrubbing and write operations.  This was done by monitoring local memory and adjusting read rates with a backoff.  Each record ( a url with tags and comments etc. ) was passed through the perl scrubbing code in the Erlang environment.  This leveraged a perl marshaling layer to pass the Erlang representation read from the queue into perl accessible formats and to convert the ‘scrubbed’ data back into Erlang types before writing them to the federation layer.  All operations were logged in a distributed in-memory database which made it easy to track progress and verify extremely low failure rates.  I was able to run 100’s of perl interpreters on a fleet boxes to increase the throughput of the overall migration process.</p>
<p>The query generator was implemented using the BOOST Spirit Recursive Descent Parser Generator.</p>
</div><h4 class="project-item third-color">
	<i>Project Deliverables</i>
</h4>

<div class="project-item-detail">
	<p>The query generator was deployed to production.  Multiple (~30) full migrations were executed.  This supported development, stress testing and verification.   During the process of launching the new Delicious architecture this system provided bi-directional replication.</p>
<p>An experience report was presented at the Commercial Users of Functional Programming  conference in 2008.</p>
<p>Developing Erlang At</p>
</div><h4 class="project-item third-color">
	<i>Current State</i>
</h4>

<div class="project-item-detail">
	<p>Yahoo! sold delicious to AVOS in 2011.</p>
</div><h4 class="project-item">
	<i>Colleagues</i>
</h4>

<table><tr>
	<td>
		Yogish Baliga:
	</td>
	<td>
		<a href="https://www.linkedin.com/in/baliga/">https://www.linkedin.com/in/baliga/</a>
	</td>
</tr>

<tr>
	<td>
		David Yan:
	</td>
	<td>
		<a href="https://www.linkedin.com/in/davidyan1/">https://www.linkedin.com/in/davidyan1/</a>
	</td>
</tr>

<tr>
	<td>
		Abe Taha:
	</td>
	<td>
		<a href="https://www.linkedin.com/in/abe-taha/">https://www.linkedin.com/in/abe-taha/</a>
	</td>
</tr>

</table>


