<h4 class="project-item third-color">
	<i>Technical Details</i>
</h4>

<div class="project-item-detail">
	<p><b>Data Migration</b></p>

<p>The database organization of the acquired company had all the warts and faults of a fast developing product.  In addition to inconsistent schema changes, there were inconsistencies in data formatting which were managed by a special layer of data munging code.  Additionally, different features on the site depended on different master/slave configurations.  The original system used over 30 database instances.  Our target was 6 clusters of master/slave triples requiring 18 databases.</p>
<center>
<p>
<img src="Technical_Details.jpg" alt="" >
</p>

<p>
<img src="Technical_Details1.jpg" alt="" >
</p>

</center>

	<ul><li>	Transform source data schema into target schema</li>
	<li>	Leverage existing data-scrubbing code - as implemented - to maintain data integrity.  </li></ul>
	<blockquote>⁃	This was achieved by running the Perl scrubbing code within Erlang processes as the data was read from the source infrastructure.</blockquote>
	<ul><li>	Dynamically scale source read / target write rates to ensure that processing hosts didn’t run out of memory due to queue backlogs.</li></ul>
	<blockquote>⁃	The Erlang VM on the processing hosts was instrumented to monitor machine memory and use an exponential back-off on the data readers to accommodate memory surges.</blockquote>
	<ul><li>	Provide robust - queryable log to track failed writes into the new installation</li></ul>
	<blockquote>⁃	Data processing logs were written to Mnesia which made it easy to aggregate processing results from multiple replication hosts and simple to query</blockquote>
	<ul><li>	Allow easy rebuilding of gold-masters as the federated database system was hardened.  We ran around 30 full migrations during testing and development.  A full migration could be achieved within ~24 hours.</li></ul>
	<blockquote>⁃	The fault tolerance capabilities of Erlang were critical, write operations were scoped to individual Erlang processes which localized failure and retry at the most granular level</blockquote>
	<ul><li>	Support a live replication from the original topology to the new system</li></ul>
	<blockquote>⁃	At launch the original database configurations handled writes and reads were served from the new infrastructure.  The migration logic maintained live parity between the two systems until stake holders were comfortable cutting writes over to the new system.</blockquote>
	<ul><li>	Provide a reverse replicator to write data from the new data sub-system to the original databases.</li></ul>
	<blockquote>⁃	This was a fail-safe.  Allow the team to cut back to the old system after writes had started flowing to the new databases.</blockquote>

<p><b>Query Generator</p>
</b><p>This code needed to map incoming URL query strings into the underlying - federation aware - SQL.  In the simple case a user was accessing their own data and this was a single query mapped to a single cluster.  Accessing tagged data from a user’s social network would require multiple concurrent queries.  There are a few more scenarios which I can no longer recall.  The query generator logic was executed in the Apache Servers providing the Federation Layer.</p>

</div>