<hr>

<h3 class="company first-color">Yahoo! Personal Finance - Banking</h3>

<table cellspacing="0">


   <tr>
		<td class="second-color"> Dates: </td>


<td> <i> Jun 2001 - Jun 2005 </i> </td>

	</tr>
 



   <tr>
		<td class="second-color"> Role: </td>

   	<td> <i> Lead </i> </td>
   </tr>



   <tr>
		<td class="second-color"> Technologies: </td>	
		<td> <i> mysql, SOAP </i> </td>
	</tr>



   <tr>
		<td class="second-color">Programming Languages: </td>

		<td> <i> c++, perl, xslt </i></td>
	</tr>
 


   <tr>
		<td class="second-color">DSLs: </td>

		<td> <i> make, sql </i></td>
	</tr>


</table>

<h4 class="project-item third-color">
	<i>Objectives</i>
</h4>

<div class="project-item-detail">
	<p>Provide a portal for personal financial history from brokerages, banks and credit cards.  Store unbounded transaction history.  Provide additional tools for budget planning and credit monitoring.</p>
</div><h4 class="project-item third-color">
	<i>Responsibilities</i>
</h4>

<div class="project-item-detail">
	<p>I was responsible for maintaining the banking infrastructure while adding new functionality.  As sole owner I was responsible for hardware, monitoring, deployment and interfacing with the product and security teams.  I was also tasked with migrating the data storage from an early Yahoo! Proprietary no-sql datastore into MySql.  During this migration I also needed to change the encryption method from Triple DES to AES.</p>

</div><h4 class="project-item third-color">
	<i>Technical Details</i>
</h4>

<div class="project-item-detail">
	<p><b><u>Data Migration</p>
</u></b><p>The data for users was encrypted with triple DES in a proprietary no-sql data store.  This was a blob store and individual transaction records were byte-delimited with a non-printable control character.  Each user’s history was encrypted with a unique encryption key.  Migration from the old to new systems was implemented so that a user’s data was migrated to MySql when they accessed their history.  Records were written to the new system using AES encryption which was the standard required by the Yahoo! Security Team ( a.k.a. Paranoids ).  Referential integrity was not leveraged in the database engine.  MyISAM tables were used as they were better understood, and had been used in production by other properties in the company.</p>
<p>A final pass was made through the full user set - using batch processing to ensure that history was fully migrated.  Batch processing was able to access user encryption keys for in-memory processing.  ( I don’t recall the full details of how this part of the mechanism worked. )</p><b><u></p>

<p>Transaction History Management</u></b></p>

<p>Transaction history was stored with an unbounded history window.  Transaction data was encrypted with a DES/3DES symmetric-key block cipher.</p>
<p>Identifying the first and last records of the merge window in the new history and the existing history needed take into account the possible changes in all three fields.  This was achieved using a sequence of rules that would be tested in decreasing order of strictness.</p>
<p><b>The Algorithm:</b></p>
	<ul><li>	Take the last record of the new data set and identify its entry in the history.</li>
	<li>	Find the lower bound of the new transaction set in the historical record</li></ul>
	<blockquote>⁃	equal match of all three fields</blockquote>
	<blockquote>⁃	equal date, equal description, different amount</blockquote>
	<blockquote>⁃	equal date, different description, equal amount</blockquote>
	<blockquote>⁃	equal date, different description, different amount ( no other entries with the same date )</blockquote>
	<blockquote>⁃	newer date, equal description, equal amount</blockquote>
	<blockquote>⁃	newer date, different description, equal amount</blockquote>
	<blockquote>⁃	newer date, equal description, different amount</blockquote>
	<blockquote>⁃	newer date, different description, different amount</blockquote>
	<blockquote>⁃	missing date in existing history range</blockquote>
	<ul><li>	Find the upper bound of the new transaction set already stored in the historical record</li></ul>
	<blockquote>⁃	Take the newest historical transaction record(s) according to date</blockquote>
	<blockquote>⁃	Use the ruleset as listed above</blockquote>
	<ul><li>	Mege the received merge set with the historical merge set</li></ul>
	<blockquote>⁃	if the cardinality of the two sets is equal</blockquote>
	<blockquote>⁃	follow the rules as above</blockquote>
	<blockquote>⁃	if the received set has a higher cardinality identify new records and add them</blockquote>
	<blockquote>⁃	follow the rules as above to isolate the new records and add</blockquote>
	<blockquote>⁃	if the received set has a lower cardinality identify deleted records and remove them</blockquote>
	<blockquote>⁃	follow the rules as above and isolate old records that should be removed</blockquote>
<p>There were further heuristics identified in practice that were added to the end of the basic rule set, eventually amounting to close to 40.  </p>
<p>The rules were implemented as functors and could be combined to form rule chains at compile time using the Loki library.  In order to develop refinements a test harness was developed that could run a ‘dry run; merge using two differently configured rule chains, this provided a way to verify if a newer ruleset improved results for edge cases as they were identified in processing or reported by a specific customer.  </p>
<p>The additional challenge was that historical records were encrypted on a per-user basis and new histories could only be processed within the security context of a given user’s credential set.</p>
</div><h4 class="project-item third-color">
	<i>Techniques Used</i>
</h4>

<div class="project-item-detail">
	<p><b>Meta-programming</b></p>
<p>The solution was implemented using C++ and the Loki meta programing library.</p>

<p><b>Composability</b></p>
<p>The various rules to match individual records were implemented as C++ functors.  These rules were organized into rule chains of decreasing strictness.  Chain processing for an individual record stopped at the first matching predicate or fell through the bottom of the chain as an unmatchable record.</p>

<p><b>Recursion</b></p>
<p>The code was implemented purely in header files.  The rule chains was generated using compiler side recursion using a technique of typelists provided by the Loki library.  This simplified rule configuration to a matter of editing a single entry in a header file.</p>
<p>These approaches provided the following additional advantages.</p>
	<ul><li>	Isolation</li></ul>
	<blockquote>⁃	Each ‘rule’ was implemented as predicate and was not coupled</blockquote>
	<ul><li>	Verification/Testing</li></ul>
	<blockquote>⁃	Multiple rulesets could be compiled with different configurations to provide efficient side-by-side comparisons.</blockquote>
	<ul><li>	Extensibilty</li></ul>
	<blockquote>⁃	Unmatched records provided the basis for ever weaker rules - these could be added to the end of a rule chain, tested and accepted or rejected.</blockquote>


</div><h4 class="project-item third-color">
	<i>Project Deliverables</i>
</h4>

<div class="project-item-detail">
	<p>The product was migrated to MySql and deployed to production.  Improved transaction history merging logic was incorporated into the system.</p>
</div><h4 class="project-item third-color">
	<i>Current State</i>
</h4>

<div class="project-item-detail">
	<p>The bank account aggregation service for banking accounts was shut down.  The current banking site serves news and advice.</p>
</div>
